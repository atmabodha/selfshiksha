{"cells":[{"cell_type":"markdown","source":["# Principal Component Analysis [PCA]"],"metadata":{"id":"Ie8pChKqzb4i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VZ3ZwoUmjjC"},"outputs":[],"source":["# References : \n","\n","# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n","\n","# https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/\n","\n","# https://shankarmsy.github.io/posts/pca-sklearn.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxLoR_iRnEm_"},"outputs":[],"source":["import pandas as pd\n","from sklearn.datasets import load_iris \n","\n","import plotly.express as px\n","\n","import numpy as np \n","import matplotlib.pyplot as plt \n","%matplotlib inline \n","\n","# Importing PCA from sklearn\n","from sklearn.decomposition import PCA \n","\n","# We also need to scale the features before applying PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlA4PH2Mxa3m"},"outputs":[],"source":["# Loading IRIS data from plotly express\n","# In Python, a lot of datasets are available from the packages itself, \n","# and there is no need to read a separate csv file\n","df = px.data.iris()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSZvE3htxa3n"},"outputs":[],"source":["# This dataset has 4 input features : sepal_length, sepal_width, petal_length and petal_width\n","# and one output variable : species (equivalent to species_id)\n","# The goal is to use the 4 input features to predict the value in the output variable (flower species)\n","# Doing this prediction is a part of supervised learning.\n","# In this PCA module, we will explore if we really need all these 4 features, or can we do with less.\n","df"]},{"cell_type":"code","source":["df[\"species\"].value_counts()"],"metadata":{"id":"zMB3dlHG-meu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"species_id\"].value_counts()"],"metadata":{"id":"xUvWFO4x-pAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Next we need to initialize the PCA module and specify the number of PCA features we are interested in.\n","# Since we have 4 input features in the data, lets first use the same number of principal components\n","pca = PCA(4) \n","print(pca)"],"metadata":{"id":"NhHFZmIky6YV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we need to prepare the data in the required format. \n","# We have 4 input features, and so we need a NumPy array with 4 columns,\n","# with each column representing an input feature.\n","\n","X = np.array(df.drop([\"species\",\"species_id\"],axis=1))"],"metadata":{"id":"sWqlBDY0zuTL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.ndim"],"metadata":{"id":"-gYk_PZo0N51"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"ChAzixRZ0PdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"NwM2rlb40KUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tm_CjsQPxDRl"},"outputs":[],"source":["# Fitting PCA to the iris dataset and transforming it into 4 principal components \n","# X_proj is again a 150x4 NumPy array which contains the input feature values transformed to the new axes.\n","X_proj = pca.fit_transform(X)\n","print(X_proj.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Mkgzlytx-jE"},"outputs":[],"source":["# What we need to know is the importance of these 4 new principal components,\n","# and for which we need to look at the variance values along these components.\n","\n","# This function tells us the extent to which each component explains the original dataset. \n","# So the 1st component is able to explain roughly 92% of the variation in the dataset \n","# and the second only about 5.3%.\n","# Together they can explain about 97.3% of the variance of X \n","# This means that we only need two principal components for further analysis.\n","print(pca.explained_variance_ratio_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0JohLujxNk3"},"outputs":[],"source":["# Plotting the 2 principal components to see if it can actually do a good job of predicting the species.\n","# As we saw earlier, these 2 components account for 97.3% of the varaince in the dataset\n","# c=y colors the scatter plot based on y (target) \n","y = df[\"species_id\"]\n","plt.scatter(X_proj[:,0], X_proj[:,1],c=y)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlNHUqr7xqS8"},"outputs":[],"source":["# pca.components_ has the actual direction of each principal component, \n","# essentially showing how it is related to the original input features.\n","# Checking shape tells us it has 4 rows, one for each principal component \n","# and 4 columns, proportion of each of the 4 features for each row.\n","print(pca.components_) "]},{"cell_type":"code","source":["print(pca.components_.shape)"],"metadata":{"id":"RjiM519n6U-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Applying PCA after scaling the input features using StandardScaler\n","\n","StandardScaler scales the input features so that they have zero mean and unit variance"],"metadata":{"id":"BB88mLbaGGoj"}},{"cell_type":"code","source":["# PCA is highly sensitive to the scale of the input feature values. \n","# We can see that the petal_width, for example, has lower values in general compared to petal_length\n","df.describe()"],"metadata":{"id":"9PACxkXTGUo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We can use the StandardScaler package from sklearn to scale all the feature values\n","sc = StandardScaler()\n","X_scaled = sc.fit_transform(X)"],"metadata":{"id":"J3_lo1AOGnmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.min(X_scaled,axis=0)"],"metadata":{"id":"S0LN3tmzGihZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.max(X_scaled,axis=0)"],"metadata":{"id":"WHk9_YneHRIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca = PCA(4)\n","X_proj = pca.fit_transform(X_scaled)\n","print(X_proj.shape)\n","print(pca.explained_variance_ratio_)\n","# Notice that without scaling, the first principal component accounted for about 92% of the variance.\n","# After scaling, the first principal component accounts for only about 72% of the variance."],"metadata":{"id":"JNTIu5AJGdr7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using MinMaxScaler\n","\n","* sklearn provides another way of scaling the input features\n","\n","* MinMaxScaler scales all input features so that their values are in the range [0,1]\n","\n","* Not preferable for PCA\n"],"metadata":{"id":"pHNY-sjqMsPB"}},{"cell_type":"code","source":["# Using MinMaxScaler\n","X_scaled = MinMaxScaler().fit_transform(X)\n","pca = PCA(4)\n","X_proj = pca.fit_transform(X_scaled)\n","print(X_proj.shape)\n","print(pca.explained_variance_ratio_)\n","# With MinMax scaling, we get a different result for the component variance as compared to StandardScaler!"],"metadata":{"id":"rQoxWTUTMDcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.min(X_scaled,axis=0)"],"metadata":{"id":"7nUwU2alMoK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.max(X_scaled,axis=0)"],"metadata":{"id":"AzcTnW8gMpNi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialising PCA to get less number of components"],"metadata":{"id":"EXFl_pmwF_AJ"}},{"cell_type":"code","source":["# Earlier we had initialized the PCA module with 4 components : pca = PCA(4)\n","# Now that we know we need only 2 components, we can use this number in our initialisation\n","# We can see that now the output is just 2 principal components which account for more than 97% of the variance.\n","\n","pca = PCA(2)\n","X_proj = pca.fit_transform(X_scaled)\n","print(X_proj.shape)\n","print(pca.explained_variance_ratio_)"],"metadata":{"id":"dfSnZYGy64Kk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialising PCA with variance specification"],"metadata":{"id":"yQFg9M0IGDo3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-33dV_BGyI9e"},"outputs":[],"source":["# While initialising the PCA module, instead of specifying the number of principal components, \n","# we can also directly specify the variance that we are looking for\n","# So if we only needed a 90% variance, we can use this:\n","pca=PCA(0.9) \n","X_proj = pca.fit_transform(X_scaled)\n","print(X_proj.shape)\n","print(pca.explained_variance_ratio_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUhDEhIUxa3s"},"outputs":[],"source":["# Instead of plotly express, you can also load IRIS data from sklearn\n","# The code is provided here just for reference and we will not be actually using it for PCA\n","\n","# iris = load_iris() \n","\n","# checking to see what datasets are available in iris \n","# iris.keys() \n","\n","# checking shape of data and list of features (X matrix) \n","# iris.data.shape\n","\n","# type(iris.data)\n","# iris.data\n","# iris.feature_names\n","# print(iris.target_names) \n","\n","# X = iris.data\n","# y = iris.target "]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":0}