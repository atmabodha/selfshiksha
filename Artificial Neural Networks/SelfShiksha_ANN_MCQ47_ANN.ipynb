{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vw9nIeDT1vkB"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics \n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.linear_model import LogisticRegression\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import Sequential\n","from keras.layers import Dense\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myoHiLACtO5f"},"outputs":[],"source":["# Download the csv with name \"SelfShiksha_ANN_MCQ40_ANN_Dataset1.csv\"\n","df=pd.read_csv(\"SelfShiksha_ANN_MCQ47_ANN_Dataset1.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYgFV5OYjnE7"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXRdAnxwjsBR"},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAr0Tx3jjw4V"},"outputs":[],"source":["df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tA_ShWitf_J"},"outputs":[],"source":["# Last column is the class label, and the remaining columns are the input feature values\n","# You can also choose a few columns as input features and analyse the results\n","\n","x=df.drop(['is_anomaly'],axis=1)\n","# x = df[['param1','param3','param6','param8','param11','param13']]\n","y=list(df['is_anomaly'])\n","\n","# This scales the input values to a suitable range\n","x = MinMaxScaler().fit_transform(x)\n","\n","# Split the dataset into a training and testing set\n","x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=4)\n","y_test = np.array(y_test)\n","y_train = np.array(y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLSbttL-tbcO"},"outputs":[],"source":["# LOGISTIC REGRESSION\n","\n","LogReg = LogisticRegression()\n","LogReg.fit(x_train,y_train)\n","y_pred=LogReg.predict(x_test)\n","accuracy = metrics.accuracy_score(y_test, y_pred)\n","print(accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4otjXayy1vkH"},"outputs":[],"source":["x_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UxEvFMV1vkH"},"outputs":[],"source":["y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4zIWvIdtdnW"},"outputs":[],"source":["#ARTIFICIAL NEURAL NETWORK\n","# Here we are defining an ANN with 3 hidden layers, each of which has 32 nodes.\n","# The hidden layers using ReLU activation and the output layer uses Sigmoid.\n","# We are using Binary Cross Entropy as our cost/loss function, \n","# and Stochastic Gradient Descent as our optimizer.\n","\n","model = Sequential()\n","model.add(Dense(32, activation='relu',input_dim = x_train.shape[1]))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1,activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='sgd',metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aV-QzEoEuDOs"},"outputs":[],"source":["# This does the actual model training.\n","# epochs refers to the number of training iterations to be used.\n","# batch_size specifies the number of data points to be used in each step of SGD.\n","\n","history = model.fit(x_train,y_train,epochs=10,shuffle=True,verbose=2,batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCGEe17suQma"},"outputs":[],"source":["loss_accuracy = model.evaluate(x_test,y_test)\n","print(\"Accuracy = \",loss_accuracy[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fMilqCouoE0"},"outputs":[],"source":["#Plottling change accuracy with epochs\n","# This is a fairly trivial problem and so the algorithm reaches almost 100% accuracy in the first epoch itself!\n","\n","plt.plot(history.history['accuracy'])\n","#plt.plot(history.history['loss'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['accuracy'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SM4PMbyBupJs"},"outputs":[],"source":["#Plotting loss values for each epoch\n","# Although the accuracy is almost 100% right from the first epoch, \n","# we can see there is a slight decrease in the loss as we train the model for more epochs.\n","\n","plt.plot(history.history['loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['loss'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bs0z48t_nXm7"},"source":["## DATASET 2"]},{"cell_type":"markdown","metadata":{"id":"3IUsAADX1vkL"},"source":["### The previous Dataset 1 was fairly simple and we got almost 100% accuracy with both Logistic Regression and ANN.\n","### Now lets take a slightly more complicated problem and see if ANN can give us better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0s8nIeX1vkL"},"outputs":[],"source":["# Source : https://www.kaggle.com/code/caghank/logistic-regression-vs-neural-network-vs-cnn/notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-m8ZXeQnWq6"},"outputs":[],"source":["from keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import cross_val_score\n","from keras.models import Sequential # initialize neural network library\n","from keras.utils import to_categorical\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","from keras.layers import Dense,Dropout, Flatten, Conv2D, MaxPool2D\n","from keras.optimizers import RMSprop,Adam,Adamax\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ReduceLROnPlateau"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQzQxXdznd7o"},"outputs":[],"source":["# Download the csv file with name \"SelfShiksha_ANN_MCQ40_ANN_Dataset2.csv\"\n","\n","df =  pd.read_csv(\"SelfShiksha_ANN_MCQ47_ANN_Dataset2.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lK8m_PeU1vkM"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqnPUX2Hnll-"},"outputs":[],"source":["# Image consists of 784 pixel which is 28 * 28 \n","\n","img_size = 28 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQt0AMnPoWjL"},"outputs":[],"source":["X = df.drop(['label'],axis = 1) \n","Y = df['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHzXjdGtoaqu"},"outputs":[],"source":["# Normalize the data\n","X = X / 255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEmQXnAroc47"},"outputs":[],"source":["#The first 25 data point in train data and their labels\n","plt.figure(figsize = (10,10))\n","for i in range (25) :\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.xlabel(Y[i])\n","    plt.imshow(X.iloc[i,:].values.reshape(img_size,img_size))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmPpZ-wMoffJ"},"outputs":[],"source":["# Divide the data into training and testing set\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n","Y_train =Y_train.values.reshape(-1,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SS0jPT0Roi9l"},"outputs":[],"source":["print(\"X_train shape is : \",X_train.shape)\n","print(\"Y_train shape is : \", Y_train.shape)\n","print(\"X_test shape is \" , X_test.shape)\n","print(\"Y_test shape is \" , Y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cln_LN4-1vkN"},"outputs":[],"source":["# LOGISTIC REGRESSION\n","\n","LogReg = LogisticRegression(solver='sag',random_state = 42,max_iter= 200, tol = 0.1,)\n","LogReg.fit(X_train,Y_train)\n","Y_pred=LogReg.predict(X_test)\n","accuracy = metrics.accuracy_score(Y_test, Y_pred)\n","print(accuracy)\n","\n","print(\"Train accuracy: {} \".format(LogReg.score(X_train, Y_train)))\n","print(\"Test accuracy: {} \".format(LogReg.score(X_test, Y_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tp4qJxGnow24"},"outputs":[],"source":["#ARTIFICIAL NEURAL NETWORK\n","\n","model = Sequential() # initialize neural network\n","model.add(Dense(units = 240, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n","model.add(Dense(units = 240, kernel_initializer = 'uniform', activation = 'relu'))\n","model.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'softmax')) #softmax is used for classification\n","\n","# adamax =Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n","\n","model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXQidwfko69p"},"outputs":[],"source":["Y_train = to_categorical(Y_train) #we are using 1 hot encoding here\n","# model.fit(X_train,Y_train,epochs=10,shuffle=True,verbose=2)\n","history = model.fit(X_train , Y_train,epochs = 10,batch_size = 150)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkoMolTWo9wq"},"outputs":[],"source":["# We can clearly see that for this dataset, ANN outperforms Logistic Regression by a significant margin!\n","\n","test_loss, test_acc = model.evaluate(X_train, Y_train) #evaluating our model\n","print('Test accuracy:', test_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6MdhXJU1vkO"},"outputs":[],"source":["# This plot shows how the model accuracy increases with increasing epochs\n","\n","plt.plot(history.history['accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['accuracy'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XfLF4Vs1vkO"},"outputs":[],"source":["# This plot shows how the model loss decreases with increasing epochs\n","\n","plt.plot(history.history['loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['loss'], loc='upper left')\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":0}