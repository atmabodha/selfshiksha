{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPPP2rkYJGCnGp1yC0UPBKI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"QT5De37EOBEX","executionInfo":{"status":"ok","timestamp":1669622028112,"user_tz":-330,"elapsed":5,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}}},"outputs":[],"source":["# Source : https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_05_embedding.ipynb"]},{"cell_type":"code","source":["from numpy import array\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Embedding, Dense"],"metadata":{"id":"Rrl5yXfnOdBa","executionInfo":{"status":"ok","timestamp":1669622038676,"user_tz":-330,"elapsed":2650,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Define 10 resturant reviews.\n","reviews = [\n","    'Never coming back!',\n","    'Horrible service',\n","    'Rude waitress',\n","    'Cold food.',\n","    'Horrible food!',\n","    'Awesome',\n","    'Awesome service!',\n","    'Rocks!',\n","    'poor work',\n","    'Couldn\\'t have done better']\n","\n","# Define labels (1=negative, 0=positive)\n","labels = array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"],"metadata":{"id":"WMV77pe9Omn0","executionInfo":{"status":"ok","timestamp":1669622046382,"user_tz":-330,"elapsed":5,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["VOCAB_SIZE = 50\n","\n","# Before feeding the input reviews to the embedding layer, \n","# we need to convert each distinct word to a number since thats what any algorithm can deal with!\n","# So here we are using text as input, but an Embedding layer can be used with any other type of data too \n","# since any data can be represented using numbers.\n","\n","encoded_reviews = [one_hot(d, VOCAB_SIZE) for d in reviews]\n","print(f\"Encoded reviews: {encoded_reviews}\")\n","\n","# So the number corresponding to the word \"Never\" is 33, the number corresponding to \"service\" is 10, and so on.\n","# The embedding layer will take these numbers as input, and learn a suitable vector representation."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjw_410UOpGo","executionInfo":{"status":"ok","timestamp":1669622057672,"user_tz":-330,"elapsed":430,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}},"outputId":"062eddc8-6cf7-465b-fd9a-5d5821085e63"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded reviews: [[33, 20, 46], [19, 10], [28, 18], [4, 8], [19, 8], [4], [4, 10], [34], [5, 35], [4, 21, 11, 19]]\n"]}]},{"cell_type":"code","source":["MAX_LENGTH = 4\n","\n","# Since the reviews have varying number of words, we need to do zero padding since all the input vectors must be of the same size.\n","padded_reviews = pad_sequences(encoded_reviews, maxlen=MAX_LENGTH,\n","                               padding='post')\n","print(padded_reviews)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"THQLlfXdOsN9","executionInfo":{"status":"ok","timestamp":1669622068115,"user_tz":-330,"elapsed":4,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}},"outputId":"1ebe990d-44bd-4179-f2e9-43493653c1cd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[33 20 46  0]\n"," [19 10  0  0]\n"," [28 18  0  0]\n"," [ 4  8  0  0]\n"," [19  8  0  0]\n"," [ 4  0  0  0]\n"," [ 4 10  0  0]\n"," [34  0  0  0]\n"," [ 5 35  0  0]\n"," [ 4 21 11 19]]\n"]}]},{"cell_type":"code","source":["model = Sequential()\n","\n","# embedding layer takes inputs of length MAX_LENGTH and has an output embedding vector of size 8. \n","# VOCAB_SIZE is the number of different numerical values the input vector elements can have.\n","# For each distinct word, the embedding layer will learn a vector representation.\n","# Since our VOCAB_SIZE is 50, and length of the embedding vector is 8, the number of parameters to be learnt is 50*8 = 400\n","\n","# Note that unlike usual hidden layers in ANN, the Embedding layer does not have any activation function!\n","\n","embedding_layer = Embedding(VOCAB_SIZE, 8, input_length=MAX_LENGTH)\n","model.add(embedding_layer)\n","\n","# This Flatten layer converts the 2D array of the Embedding layer to a 1D array.\n","# Since the initial text input is a vector of size 4, and the embedding layer learns a vector of size 8 for each word, \n","# the flatten layer output will be a vector of size 32.\n","model.add(Flatten())\n","\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GI8aLh5XOu5V","executionInfo":{"status":"ok","timestamp":1669622079613,"user_tz":-330,"elapsed":686,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}},"outputId":"dea1c16f-1e78-437b-8373-7c64e22536d7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 4, 8)              400       \n","                                                                 \n"," flatten (Flatten)           (None, 32)                0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 433\n","Trainable params: 433\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["# fit the model\n","model.fit(padded_reviews, labels, epochs=100, verbose=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KrW9Xy68Oxmt","executionInfo":{"status":"ok","timestamp":1669622095691,"user_tz":-330,"elapsed":1615,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}},"outputId":"210b17ff-0695-4c16-ba02-ad89e173e3e3"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f552bf2ba90>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["print(embedding_layer.get_weights()[0].shape)\n","# print(embedding_layer.get_weights())\n","\n","# The number corresponding to the word \"Never\" is 33, the number corresponding to \"service\" is 10, and so on.\n","print(\"\\n\\n\")\n","print(\"Embedding vector for the word Never is:\\n\",embedding_layer.get_weights()[0][33])\n","print(\"\\n\\n\")\n","print(\"Embedding vector for the word service is:\\n\",embedding_layer.get_weights()[0][10])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QF2YJn1tO1R7","executionInfo":{"status":"ok","timestamp":1669622871360,"user_tz":-330,"elapsed":7,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}},"outputId":"d7e7721b-bf2c-4e9b-81c6-d0b3c64555a4"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["(50, 8)\n","\n","\n","\n","Embedding vector for the word Never is:\n"," [-0.08492238  0.14234544  0.06741757  0.08559477  0.05124443 -0.10405801\n"," -0.1454268  -0.06898103]\n","\n","\n","\n","Embedding vector for the word service is:\n"," [-0.06222677 -0.03977245  0.01772755 -0.01145729 -0.05897132  0.09337803\n","  0.01182112 -0.07200011]\n"]}]},{"cell_type":"code","source":["loss, accuracy = model.evaluate(padded_reviews, labels, verbose=0)\n","print(f'Accuracy: {accuracy}')\n","\n","# This model is clearly overfitting since we have only a few reviews in our training data, and there is no overlap between the words in any review. \n","# The accuracy will drop when the same word appears in both positive and negative reviews. Try it out and see what happens."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QNyKziXxO4RY","executionInfo":{"status":"ok","timestamp":1669622118048,"user_tz":-330,"elapsed":712,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}},"outputId":"33e53a4f-a4e8-4200-95a5-f3a7fb544be7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n"]}]},{"cell_type":"code","source":["print(f'Log-loss: {loss}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSh4sqU9O7DL","executionInfo":{"status":"ok","timestamp":1669622132267,"user_tz":-330,"elapsed":6,"user":{"displayName":"Kushal Shah","userId":"02499066412137853896"}},"outputId":"19bd7e18-27dd-4a61-a89d-34af028adcc4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Log-loss: 0.46238845586776733\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"v1vGOg-EO-hX"},"execution_count":null,"outputs":[]}]}